{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved successfully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "# Check if the camera is opened successfully\n",
    "if not capture.isOpened():\n",
    "    print(\"Unable to open the camera\")\n",
    "\n",
    "# Read a frame from the camera\n",
    "ret, frame = capture.read()\n",
    "\n",
    "# Check if the frame is read successfully\n",
    "if not ret:\n",
    "    print(\"Failed to read frame from the camera\")\n",
    "\n",
    "cv2.imwrite(\"../testing_data/A.jpg\", frame)\n",
    "print(\"Image saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_frame():\n",
    "    frame = cv2.imread(\"../testing_data/A.jpg\")\n",
    "    return frame\n",
    "\n",
    "\n",
    "def apply_filters(frame):\n",
    "    # Apply Canny filter\n",
    "    # frame = cv2.Canny(frame, 100, 200)\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    hsv[:, :, 2] = cv2.equalizeHist(hsv[:, :, 2])\n",
    "\n",
    "    lower_skin = np.array([0, 48, 80], dtype=np.uint8)\n",
    "    upper_skin = np.array([150, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "    # Apply opening to the mask\n",
    "    mask_smoothed = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    edges = cv2.Canny(gray, 25, 150)\n",
    "\n",
    "    # lines = cv2.HoughLinesP(\n",
    "    #     edges, 1, np.pi / 180, 75, minLineLength=0.01, maxLineGap=5\n",
    "    # )\n",
    "\n",
    "    # for line in lines:\n",
    "    #     x1, y1, x2, y2 = line[0]\n",
    "    #     cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    return edges\n",
    "\n",
    "\n",
    "def filter_hands(frame):\n",
    "    # Load the Haar cascade xml file for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    )\n",
    "\n",
    "    # Convert the image to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Perform face detection\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # If a face is detected\n",
    "    if len(faces) > 0:\n",
    "        # Get the coordinates of the face\n",
    "        (x, y, w, h) = faces[0]\n",
    "\n",
    "        # Set the region of the image that contains the face to black\n",
    "        frame[\n",
    "            y - 30 : y + h + 50,\n",
    "            x - 10 : x + w + 10,\n",
    "        ] = 0\n",
    "\n",
    "        frame[\n",
    "            y - 30 : y + h + 85,\n",
    "            x + 10 : x + w - 30,\n",
    "        ] = 0\n",
    "    # frame = gray\n",
    "\n",
    "    # Convert the image to the HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define a range for skin color in HSV\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    # Threshold the HSV image to get only skin colors\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter the contours based on their area\n",
    "    hand_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 5000]\n",
    "\n",
    "    # # Draw the hand contours on the frame\n",
    "    # cv2.drawContours(frame, hand_contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "    # Create a blank black image\n",
    "    hand_mask = np.zeros(frame.shape[:2], dtype=\"uint8\")\n",
    "\n",
    "    # # Fill in the hand contours with white\n",
    "    # cv2.drawContours(hand_mask, hand_contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    # Fill in the hand contours with white\n",
    "    cv2.drawContours(hand_mask, hand_contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    # Create a mask where everything outside of the hand contours is black\n",
    "    # outside_mask = cv2.bitwise_not(hand_mask)\n",
    "\n",
    "    # Set everything outside of the hand contours to black in the original image\n",
    "    frame = cv2.bitwise_and(frame, frame, mask=hand_mask)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def apply_image_filters(frame):\n",
    "    hands = filter_hands(frame)\n",
    "    filtered_frame = apply_filters(hands)\n",
    "\n",
    "    return filtered_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example image for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(\"../testing_data/A.jpg\")\n",
    "\n",
    "hands = filter_hands(frame)\n",
    "filtered_frame = apply_filters(hands)\n",
    "\n",
    "# Display the original image and the edges\n",
    "cv2.imshow(\"Original Image\", frame)\n",
    "cv2.imshow(\"Hands filtered\", hands)\n",
    "cv2.imshow(\"Edges\", filtered_frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# live run of filtered image for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the video source or camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the video source or camera is opened successfully\n",
    "if not capture.isOpened():\n",
    "    print(\"Unable to open the video source or camera\")\n",
    "\n",
    "# Loop through the frames\n",
    "while True:\n",
    "    # Read a frame from the video source or camera\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    hands = filter_hands(frame)\n",
    "    filtered_frame = apply_filters(hands)\n",
    "\n",
    "    # Check if the frame is read successfully\n",
    "    if not ret:\n",
    "        print(\"Failed to read frame from the video source or camera\")\n",
    "        break\n",
    "\n",
    "    # Perform operations on the frame\n",
    "    # For example, you can display the frame\n",
    "    cv2.imshow(\"Frame\", filtered_frame)\n",
    "\n",
    "    # Exit the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video source or camera and close all windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# (0) in VideoCapture is used to connect to your computer's default camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing current time and previous time for calculating the FPS\n",
    "previousTime = 0\n",
    "currentTime = 0\n",
    "\n",
    "\n",
    "previous_capture_time = time.time()\n",
    "capturing = False\n",
    "\n",
    "\n",
    "current_class = 0\n",
    "sequences_captured = 0\n",
    "capture_frame = 0\n",
    "\n",
    "paused = -1\n",
    "\n",
    "collection_classes = [\n",
    "    # \"my\",\n",
    "    # \"name\",\n",
    "    \"hello\",\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    # \"D\": 2,\n",
    "    # \"A\": 2,\n",
    "    # \"N\": 2,\n",
    "    # \"I\": 2,\n",
    "    # \"E\": 2,\n",
    "    # \"L\": 2,\n",
    "]\n",
    "\n",
    "\n",
    "DATA_PATH = \"../testing_data/\"\n",
    "\n",
    "CSV_FILE = \"../testing_data/training_data.csv\"\n",
    "\n",
    "csv_file = open(CSV_FILE, \"w\")\n",
    "\n",
    "capture_frames = 20\n",
    "capture_interval = 1.5\n",
    "n_sequences = 50\n",
    "\n",
    "while capture.isOpened():\n",
    "    # capture frame by frame\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # resizing the frame for better view\n",
    "    image = cv2.resize(frame, (800, 600))\n",
    "\n",
    "    # Converting the from BGR to RGB\n",
    "    image = apply_image_filters(image)\n",
    "\n",
    "    if not capturing:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if paused == 1:\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                \"paused, press p to continue\",\n",
    "                (215, 350),\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                1,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "            )\n",
    "        else:\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                \"Get ready to capture\",\n",
    "                (225, 350),\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                1,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "            )\n",
    "            if time.time() - previous_capture_time >= capture_interval:\n",
    "                capturing = True\n",
    "\n",
    "    if capturing:\n",
    "        # Making predictions using holistic model\n",
    "        # To improve performance, image is marked as not writeable to pass by reference.\n",
    "\n",
    "        folder_path = (\n",
    "            f\"{DATA_PATH}/{collection_classes[current_class]}/{sequences_captured}\"\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        resized_frame = cv2.resize(image, (256, 256))\n",
    "\n",
    "        flattened_image = resized_frame.flatten()\n",
    "\n",
    "        if capture_frame > 0:\n",
    "            cv2.imwrite(folder_path + f\"/{capture_frame}.jpg\", resized_frame)\n",
    "\n",
    "            csv_file.write(\n",
    "                f\"{current_class},{','.join([str(pixel) for pixel in flattened_image])}\\n\"\n",
    "            )\n",
    "\n",
    "\n",
    "        capture_frame += 1\n",
    "\n",
    "        if capture_frame == capture_frames:\n",
    "            capturing = False\n",
    "            previous_capture_time = time.time()\n",
    "            capture_frame = 0\n",
    "            sequences_captured += 1\n",
    "\n",
    "            if sequences_captured == n_sequences:\n",
    "                sequences_captured = 0\n",
    "                current_class += 1\n",
    "                if current_class == len(collection_classes):\n",
    "                    print(\"Data collection complete\")\n",
    "                    break\n",
    "\n",
    "    # Calculating the FPS\n",
    "    currentTime = time.time()\n",
    "    fps = 1 / (currentTime - previousTime)\n",
    "    previousTime = currentTime\n",
    "\n",
    "    # Displaying FPS on the image\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        str(int(fps)) + \" FPS\",\n",
    "        (10, 70),\n",
    "        cv2.FONT_HERSHEY_COMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        \"capturing \"\n",
    "        + str(sequences_captured + 1)\n",
    "        + \" of \"\n",
    "        + str(n_sequences)\n",
    "        + ' for \"'\n",
    "        + str(collection_classes[current_class])\n",
    "        + '\"',\n",
    "        (10, 575),\n",
    "        cv2.FONT_HERSHEY_COMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        \"class \" + str(current_class + 1) + \" of \" + str(len(collection_classes)),\n",
    "        (525, 575),\n",
    "        cv2.FONT_HERSHEY_COMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "    # Enter key 'q' to break the loop\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    quit_key = ord(\"q\")\n",
    "    paused_key = ord(\"p\")\n",
    "\n",
    "    if key == quit_key:\n",
    "        break\n",
    "    elif key == paused_key:\n",
    "        paused *= -1\n",
    "        previous_capture_time = time.time()\n",
    "\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the seuqences and preprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "SCALER = StandardScaler()\n",
    "ENCODER = LabelEncoder()\n",
    "\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "data = pd.read_csv(CSV_FILE)  # skiprows=1, header=None)\n",
    "\n",
    "# Create a dictionary mapping class names to their index\n",
    "class_index_mapping = {class_name: index for index, class_name in enumerate(collection_classes)}\n",
    "\n",
    "# Map the class_name column to the index of the dictionary value\n",
    "data.iloc[:, 0] = data.iloc[:, 0].map(class_index_mapping)\n",
    "\n",
    "labels = ENCODER.fit_transform(data.pop(\"class_name\"))\n",
    "\n",
    "\n",
    "data = np.insert(data, 0, labels, axis=1)\n",
    "\n",
    "# Save the processed data to a new file, excluding the first row\n",
    "processed_data_filename = \"processed_pose_attributes.csv\"\n",
    "np.savetxt(processed_data_filename, data, delimiter=\",\")\n",
    "# data.to_csv(processed_data_filename, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 2., 2., 3., 1., 1., 3., 3., 0., 3., 1., 3., 0., 1., 3., 1.,\n",
      "        3., 2., 1., 2., 0., 1., 1., 0., 2., 2., 1., 3., 3., 1., 2., 3., 2., 3.,\n",
      "        0., 1., 1., 1., 1., 0., 3., 2., 2., 3., 2.])\n",
      "torch.Size([47, 20, 65536])\n",
      "torch.Size([47])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class HandDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.labels = data[:, :, 0][:, 0]  # Extract the label column\n",
    "        self.data = data[:, :, 1:]  # Exclude the data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"data\": torch.tensor(self.data[idx], dtype=torch.float32),\n",
    "            \"label\": torch.tensor(\n",
    "                self.labels[idx], dtype=torch.float32\n",
    "            ),  # .reshape(1),\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Load the processed data from the CSV file\n",
    "processed_data = np.loadtxt(CSV_FILE, delimiter=\",\")\n",
    "\n",
    "image_size = 256 * 256\n",
    "\n",
    "reshaped_data = processed_data.reshape(-1, capture_frames, image_size + 1)\n",
    "\n",
    "# Create a dataset from the processed data\n",
    "dataset = HandDataset(reshaped_data)\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create data loaders for training and testing sets\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Get the indices of the data and split them\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.75, random_state=42)\n",
    "\n",
    "# Create subsets based on the split indices\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "training_batch_size = 100\n",
    "test_batch_size = 25\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=training_batch_size, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "# Loop through each batch in the training set\n",
    "for batch in train_dataloader:\n",
    "    data = batch[\"data\"]\n",
    "    label = batch[\"label\"]\n",
    "    print(label)\n",
    "    print(data.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        # print(x.shape)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.3790, Validation Accuracy: 0.3103\n",
      "Epoch [2/10], Loss: 1.2682, Validation Accuracy: 0.4828\n",
      "Epoch [3/10], Loss: 1.1955, Validation Accuracy: 0.4828\n",
      "Epoch [4/10], Loss: 1.1280, Validation Accuracy: 0.6207\n",
      "Epoch [5/10], Loss: 1.0662, Validation Accuracy: 0.6552\n",
      "Epoch [6/10], Loss: 1.0135, Validation Accuracy: 0.6897\n",
      "Epoch [7/10], Loss: 0.9479, Validation Accuracy: 0.7586\n",
      "Epoch [8/10], Loss: 0.8994, Validation Accuracy: 0.7931\n",
      "Epoch [9/10], Loss: 0.8325, Validation Accuracy: 0.8276\n",
      "Epoch [10/10], Loss: 0.7787, Validation Accuracy: 0.9310\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load your CSV file without a header\n",
    "df = pd.read_csv(CSV_FILE, header=None)\n",
    "\n",
    "# Extract labels and keypoint data columns\n",
    "labels = df.iloc[:, 0].astype(\"category\").cat.codes.values\n",
    "keypoints = df.iloc[:, 1:]\n",
    "\n",
    "# Normalize keypoints if needed (you might want to customize this based on your data)\n",
    "scaler = StandardScaler()\n",
    "keypoints_normalized = scaler.fit_transform(keypoints)\n",
    "\n",
    "# Reshape the data into sequences of 30 frames\n",
    "keypoints_reshaped = keypoints_normalized.reshape(\n",
    "    -1, capture_frames, len(keypoints.columns)\n",
    ")\n",
    "\n",
    "# Ensure the number of samples in keypoints_reshaped matches the number of labels\n",
    "num_samples = min(len(labels), len(keypoints_reshaped))\n",
    "keypoints_reshaped = keypoints_reshaped[:num_samples]\n",
    "labels = labels[::capture_frames]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "keypoints_train, keypoints_val, labels_train, labels_val = train_test_split(\n",
    "    keypoints_reshaped, labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, keypoints, labels):\n",
    "        self.keypoints = keypoints\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.keypoints[idx]), torch.LongTensor(\n",
    "            [self.labels[idx]]\n",
    "        )\n",
    "\n",
    "\n",
    "# Create instances of the dataset for training and validation\n",
    "train_dataset = KeypointDataset(keypoints_train, labels_train)\n",
    "val_dataset = KeypointDataset(keypoints_val, labels_val)\n",
    "\n",
    "# Create DataLoader instances\n",
    "training_batch_size = 85\n",
    "validation_batch_size = 15\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=validation_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# # LSTM Model\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = self.fc(out[:, -1, :])  # Only take the output from the last time step\n",
    "#         return out\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, hidden_size, num_layers, num_classes, dropout_prob=0.2\n",
    "    ):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Multiply by 2 for BiLSTM\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :])  # Only take the output from the last time step\n",
    "        return out\n",
    "\n",
    "\n",
    "# class CNNModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNNModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "#         self.fc1 = nn.Linear(9216, 128)\n",
    "#         self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         output = F.log_softmax(x, dim=1)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# Set model parameters\n",
    "input_size = len(keypoints.columns)  # Number of keypoints in each frame\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = len(df.iloc[:, 0].unique())  # Number of unique classes\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for data, labels in val_loader:\n",
    "            outputs = model(data)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # print(labels.squeeze(1))\n",
    "            # print(preds)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy on the validation set\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {accuracy:.4f}\"\n",
    "    )\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/lstm_image_rnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise labels and predictions from training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'A', 'B', 'C']\n",
      "tensor([[3],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1]])\n",
      "tensor([3, 3, 2, 1, 2, 0, 0, 0, 2, 2, 2, 2, 0, 3, 1])\n",
      "tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([2, 3, 2, 1, 0, 0, 0, 3, 2, 1, 2, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(collection_classes)\n",
    "\n",
    "for data, labels in val_loader:\n",
    "    outputs = model(data)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    print(labels)\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and test live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Set initial hidden and cell states\n",
    "#         # print(x.shape)\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "#         # Forward propagate LSTM\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, hidden_size, num_layers, num_classes, dropout_prob=0.2\n",
    "    ):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Multiply by 2 for BiLSTM\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :])  # Only take the output from the last time step\n",
    "        return out\n",
    "\n",
    "\n",
    "# load the model\n",
    "# Set model parameters\n",
    "input_size = 256 * 256\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = len(collection_classes)\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "model.load_state_dict(torch.load(\"../models/lstm_image_rnn.pth\"))\n",
    "model.eval()\n",
    "\n",
    "SCALER = StandardScaler()\n",
    "ENCODER = LabelEncoder()\n",
    "\n",
    "device = torch.device(\"cpu\")  # \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "frame_buffer = []\n",
    "output_buffer = []\n",
    "\n",
    "\n",
    "# Function to process the frame and make predictions\n",
    "def process_frame(image):\n",
    "    global frame_buffer, output_buffer\n",
    "\n",
    "    resized_frame = cv2.resize(image, (256, 256))\n",
    "\n",
    "    flattened_image = resized_frame.flatten()\n",
    "\n",
    "    frame_buffer.append(flattened_image)\n",
    "\n",
    "    # Maintain only the last 5 frames in the buffer\n",
    "    if len(frame_buffer) >= capture_frames:\n",
    "        frame_buffer = frame_buffer[-capture_frames:]\n",
    "\n",
    "    if len(frame_buffer) == capture_frames:\n",
    "        # df = pd.DataFrame(frame_buffer)\n",
    "\n",
    "        # input_tensor = torch.tensor(df, dtype=torch.float32)\n",
    "        # input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "        values = np.array(frame_buffer)\n",
    "        input_tensor = torch.tensor(values, dtype=torch.float32)\n",
    "        input_tensor = input_tensor.view(1, capture_frames, -1).to(\n",
    "            device\n",
    "        )  # Reshape the input tensor\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            output_buffer.append(output)\n",
    "\n",
    "            _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "            return collection_classes[predicted_class]\n",
    "\n",
    "\n",
    "displayed_prediction = None\n",
    "previousTime = 0\n",
    "previousTime_1 = 0\n",
    "\n",
    "# Open the video capture\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop through the frames\n",
    "while capture.isOpened():\n",
    "    # Read the frame from the video capture\n",
    "    ret, image = capture.read()\n",
    "\n",
    "    # resizing the frame for better view\n",
    "    image = cv2.resize(image, (800, 600))\n",
    "\n",
    "    filtered_image = apply_image_filters(image)\n",
    "\n",
    "    # Process the frame and make predictions\n",
    "    prediction = process_frame(filtered_image)\n",
    "\n",
    "    visual_image = cv2.cvtColor(filtered_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    if prediction is not None:\n",
    "        # Draw the predictions on the frame (if required)\n",
    "        cv2.putText(\n",
    "            visual_image,\n",
    "            f\"Prediction: {prediction}\",\n",
    "            (10, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    # Calculating the FPS\n",
    "    fps = 1 / (time.time() - previousTime)\n",
    "    previousTime = time.time()\n",
    "\n",
    "    # Displaying FPS on the image\n",
    "    cv2.putText(\n",
    "        visual_image,\n",
    "        str(int(fps)) + \" FPS\",\n",
    "        (10, 70),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow(\"Frame\", visual_image)\n",
    "\n",
    "    # Check for key press to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "\n",
    "# Release the video capture and close the window\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
